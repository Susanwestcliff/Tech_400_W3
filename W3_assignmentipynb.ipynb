{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import math"
      ],
      "metadata": {
        "id": "8qA4l1WXwhl5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize the text\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Function to calculate term frequency (TF)\n",
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document)\n",
        "\n",
        "# Function to calculate inverse document frequency (IDF)\n",
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "GX1HqmmuSSXV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute TF-IDF for a document\n",
        "def compute_tfidf(document, all_documents, vocabulary):\n",
        "    tfidf_vector = []\n",
        "    for term in vocabulary:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, all_documents)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return np.array(tfidf_vector)\n",
        "\n",
        "# Function to compute cosine similarity between two vectors\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    norm_vector1 = np.linalg.norm(vector1)\n",
        "    norm_vector2 = np.linalg.norm(vector2)\n",
        "    return dot_product / (norm_vector1 * norm_vector2)"
      ],
      "metadata": {
        "id": "7wfQYKqUXYv8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "  #data\n",
        "    data_directory = '/content/data'\n",
        "\n",
        "    # Reading all files from the directory\n",
        "    documents = []\n",
        "    file_names = []\n",
        "    for file_name in os.listdir(data_directory):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            with open(os.path.join(data_directory, file_name), \"r\") as file:\n",
        "                content = file.read()\n",
        "                documents.append(content)\n",
        "                file_names.append(file_name)\n",
        "\n",
        "    # Hardcoded queries\n",
        "    search_queries = ['The three women sit huddled together',\n",
        "    'The cave remained a place',\n",
        "    'That leads some here to worry',\n",
        "    'artificial intelligence model that',\n",
        "    'The planet was really vulnerable at that time']\n",
        "\n",
        "    # Tokenizing documents and queries\n",
        "    tokenized_documents = [tokenize(doc) for doc in documents]\n",
        "    tokenized_queries = [tokenize(query) for query in search_queries]\n",
        "\n",
        "    # Building the vocabulary (unique words across all documents)\n",
        "    vocabulary = sorted(set([word for doc in tokenized_documents for word in doc]))\n",
        "\n",
        "  # Calculate TF-IDF vectors for documents and queries\n",
        "    document_tfidf_vectors = [compute_tfidf(doc, tokenized_documents, vocabulary) for doc in tokenized_documents]\n",
        "    query_tfidf_vectors = [compute_tfidf(query, tokenized_documents, vocabulary) for query in tokenized_queries]\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarity_results = []\n",
        "    for query_vector in query_tfidf_vectors:\n",
        "        similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in document_tfidf_vectors]\n",
        "        similarity_results.append(similarities)\n",
        "\n",
        "    # Write the results to a text file\n",
        "    with open(\"similarity_results_susan.txt\", \"w\") as output_file:\n",
        "        for i, query in enumerate(search_queries):\n",
        "            output_file.write(f\"\\nCosine similarities for query '{query}':\\n\")\n",
        "            for j, doc in enumerate(documents):\n",
        "                output_file.write(f\"Document {file_names[j]}: {similarity_results[i][j]:.4f}\\n\")\n",
        "\n",
        "    # Optional: print results for checking\n",
        "    for i, query in enumerate(search_queries):\n",
        "        print(f\"\\nCosine similarities for query '{query}':\")\n",
        "        for j, doc in enumerate(documents):\n",
        "            print(f\"Document {file_names[j]}: {similarity_results[i][j]:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV1VL0puXZIj",
        "outputId": "5c7ab705-d240-416d-b0ee-e7f2beabbf07"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cosine similarities for query 'The three women sit huddled together':\n",
            "Document Data9.txt: 0.0068\n",
            "Document data6.txt: 0.0079\n",
            "Document data2.txt: 0.0030\n",
            "Document Data5.txt: 0.0037\n",
            "Document Data8.txt: 0.0056\n",
            "Document data4.txt: 0.0333\n",
            "Document data1.txt: 0.0691\n",
            "Document data3.txt: 0.0212\n",
            "Document Data7.txt: 0.0084\n",
            "Document Data10.txt: 0.0114\n",
            "\n",
            "Cosine similarities for query 'The cave remained a place':\n",
            "Document Data9.txt: 0.0832\n",
            "Document data6.txt: 0.0219\n",
            "Document data2.txt: 0.0059\n",
            "Document Data5.txt: 0.0082\n",
            "Document Data8.txt: 0.0125\n",
            "Document data4.txt: 0.0086\n",
            "Document data1.txt: 0.0078\n",
            "Document data3.txt: 0.0062\n",
            "Document Data7.txt: 0.0087\n",
            "Document Data10.txt: 0.0076\n",
            "\n",
            "Cosine similarities for query 'That leads some here to worry':\n",
            "Document Data9.txt: 0.0152\n",
            "Document data6.txt: 0.0033\n",
            "Document data2.txt: 0.0024\n",
            "Document Data5.txt: 0.0807\n",
            "Document Data8.txt: 0.0040\n",
            "Document data4.txt: 0.0063\n",
            "Document data1.txt: 0.0044\n",
            "Document data3.txt: 0.0033\n",
            "Document Data7.txt: 0.0027\n",
            "Document Data10.txt: 0.0068\n",
            "\n",
            "Cosine similarities for query 'artificial intelligence model that':\n",
            "Document Data9.txt: 0.0153\n",
            "Document data6.txt: 0.0424\n",
            "Document data2.txt: 0.1114\n",
            "Document Data5.txt: 0.0027\n",
            "Document Data8.txt: 0.0014\n",
            "Document data4.txt: 0.0021\n",
            "Document data1.txt: 0.0015\n",
            "Document data3.txt: 0.0247\n",
            "Document Data7.txt: 0.0008\n",
            "Document Data10.txt: 0.0006\n",
            "\n",
            "Cosine similarities for query 'The planet was really vulnerable at that time':\n",
            "Document Data9.txt: 0.0097\n",
            "Document data6.txt: 0.0875\n",
            "Document data2.txt: 0.0070\n",
            "Document Data5.txt: 0.0076\n",
            "Document Data8.txt: 0.0101\n",
            "Document data4.txt: 0.0104\n",
            "Document data1.txt: 0.0364\n",
            "Document data3.txt: 0.0082\n",
            "Document Data7.txt: 0.0084\n",
            "Document Data10.txt: 0.0164\n"
          ]
        }
      ]
    }
  ]
}